# Datasets Selection

调研数据集主要考虑加点：

- 在Uncertainty相关论文中是否常见，或者被VQA广泛采用

- **Corpus:**是否具备相应的检索语料库
- **Format：**数据集格式，非JSON格式需要转换成JSON格式以使用FlashRAG


| Datasets         | Answer Type                      | Corpus                         | Format  | Link                                                         |
| ---------------- | -------------------------------- | ------------------------------ | ------- | ------------------------------------------------------------ |
| ConBench         | True/False<br />Choice<br />Open | No                             | JSON    | Leaderboard/Eval:https://github.com/foundation-multimodal-models/ConBench<br />https://huggingface.co/datasets/ConBench/ConBench |
| MME              | Yes/No                           | No                             | PARQUET | https://huggingface.co/datasets/darkyarding/MME              |
| MMMU             | Multi-choice<br />Open           | No                             | PARQUET | Leaderboard/Eval:https://github.com/MMMU-Benchmark/MMMU<br />https://huggingface.co/datasets/MMMU/MMMU |
| MMBench          | Choice                           | No                             | PARQUET | https://huggingface.co/datasets/lmms-lab/MMBench             |
| MCRAG            | Open                             | KG                             | JSONL   | https://huggingface.co/datasets/crag-mm-2025/crag-mm-single-turn-public/tree/main/data |
| SEED-Bench       | Choice                           | No                             | JSONL   | https://huggingface.co/datasets/AILab-CVC/SEED-Bench-H       |
| MobileVQA        | Open                             | No                             | PARQUET | https://huggingface.co/datasets/arnaudstiegler/mobile_capture_vqa |
| Path-VQA         | Yes/No<br />Open                 | No                             | PARQUET | https://huggingface.co/datasets/flaviagiammarino/path-vqa    |
| July24-NewsVQA   | Choice                           | No                             | JSON    | https://huggingface.co/datasets/SsssOvO/July24-NewsVQA       |
| VQAv2            | Yes/No                           | No                             | JSON    | https://visualqa.org/download.html                           |
| GQA              | Open<br />binary                 | KG                             | PQRQUET | https://visualreasoning.net                                  |
| NeXT-GQA/Next-QA | Open                             | No                             | JSON    | https://github.com/doc-doc/NExT-GQA                          |
| VizWiz           | Open<br />binary                 | No                             | JSON    | https://vizwiz.org/                                          |
| TextVQA          | Open                             | No                             | JSON    | https://dl.fbaipublicfiles.com/textvqa/data/                 |
| ScienceQA        | Choice                           | No                             | PARQUET | https://huggingface.co/datasets/derek-thomas/ScienceQA/tree/main/data |
| OK-VQA           | Open                             | No                             | PARQUET | https://huggingface.co/datasets/lmms-lab/OK-VQA              |
| A-OKVQA          | Open                             | No                             | PQRQUET | https://huggingface.co/datasets/HuggingFaceM4/A-OKVQA        |
| S3VQA            | Open                             | No                             | JSON    | https://s3vqa.github.io/                                     |
| ViQuAE           | Open                             | No                             | JSON    | https://huggingface.co/datasets/PaulLerner/viquae_dataset/tree/main |
| InfoSeek         | Open                             | 提供了问题与Wiki词条的链接对应 | JSON    | https://github.com/open-vision-language/infoseek             |
| Dyn-VQA          | Open                             | No                             | JSON    | https://huggingface.co/datasets/zhzhen23/DynVQA              |
| WebQA            | Open                             | No                             | JSON    | https://github.com/WebQnA/WebQA/blob/main/demo/Take_a_look_WebQA.ipynb |

- 最终选取OK-VQA，因为出现在相关的重要工作中的频率较高，答案类型是开放式回答。

- 语料库的问题，最终选择Wikipedia作为语料库。理由如下：
  - OK-VQA论文使用了Wikipedia作为了测试模型的基线
  - Wikipedia是非结构化的真实世界的知识，与OK-VQA的设计初衷一致。
  - 尽管论文中表明使用Wikipedia作为外部知识来源是不够的，因为回答数据集大部分数据是需要Wikipedia之外的常识性或者视觉数据，但是常识性数据应该交给生成器组件的推理能力，视觉数据应该交给骨干模型的视觉推理能力。

# 构建Dummy OK-VQA

**Step 1. **从`lmms-lab/OK-VQA`中随机抽样100个数据。

**Step2. **修改数据格式为

```json
new_item = {
        "id": old_item["question_id"],
        "question": old_item["question"],
        "image_id": old_item["question_id"],
        "golden_answers": old_item["answers"]
    }
```

使其适配FlashRAG。

**Step3.** 将其保存为`jsonl`格式，由于json格式不能保存`Image`对象，所以将100个数据中的`Image`单独保存在一个文件夹中，并在数据点中增加`image_id`项，该项与`question_id`一致，用于唯一标识图像并建立与图像的链接。每张图像的文件名为`{image_id}.jpg`。

最终`dummy`数据集文件结构为：

```shell
├── data
│   ├── images
│   │   └── val2014
│   │       ├── 1025945.jpg
│   │       ├── 1044945.jpg
│   │       └── ...
│   └── okvqa_dummy_100
│       └── dev.jsonl
├── indexes
│   └── bm25
│       ├── corpus.jsonl
│       ├── corpus.mmindex.json
│       ├── data.csc.index.npy
│       ├── indices.csc.index.npy
│       ├── indptr.csc.index.npy
│       ├── params.index.json
│       ├── stopwords.tokenizer.json
│       ├── vocab.index.json
│       └── vocab.tokenizer.json
```

# 基于FlashRAG Codebase实现OmniSearch

## 修改内容

一、检索逻辑

只进行Text Retrieval

二、修改的文件及修改点

1. 启动脚本`run_omni_pipeline.py`：运行OmniSearch。
2. 创建`my_config.yaml`。
3. `generator/multimoal_generator.py`：类`Qwen2VLInferenceEngine`的`generate`方法，使其适应`Qwen2_5VLProcessor`推理方法。
4. `pipeline/mm_pipeline.py`：增加`OmniSearchPipeline。`
5. `prompt/mm_prompt.py`：修改格式化prompt方法`get_string()`，使其满足`OmniSearch`Prompt的要求；同时增加将数据集中图片的路径解析为`PIL.Image`类型。
6. `retriever/retriever.py`：DEBUG，直接传入`query`列表而不是再一次封装成列表。
7. `utils/utils.py`：无实质性修改。

​	
